{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Копия блокнота \"01_SegmentTokenizePoS.ipynb\"",
      "provenance": [],
      "collapsed_sections": [
        "DPMiIDMEeDbE",
        "dop3xhkteDbN",
        "gqd_IVNNeDbS",
        "BN71xIw6eDbY",
        "NDIt0zSKeDbg",
        "PhBHsBTyeDbo"
      ]
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scB5smameDYo",
        "colab_type": "text"
      },
      "source": [
        "# Настройка\n",
        "Все зависимости перечислены в ячейке ниже. Кроме того, есть ещё дополнительные данные (opencorpora, например). Они тоже скачиваются в первых ячейках."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PoyIMujeDYp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9a86e3fd-323f-425e-baac-582d412941d2"
      },
      "source": [
        "%%writefile requirements.txt\n",
        "nltk>=3.4.5\n",
        "razdel>=0.4.0\n",
        "rusenttokenize>=0.0.5\n",
        "b-labs-models>=2017.8.22\n",
        "lxml>=4.2.1\n",
        "spacy>=2.1.4\n",
        "pymystem3>=0.2.0\n",
        "rnnmorph>=0.4.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Writing requirements.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3ysaxzVeDYt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "6f0efa5b-7635-4e8b-a4aa-2dea14a0c102"
      },
      "source": [
        "import sys\n",
        "!pip install --user --upgrade --force-reinstall -r requirements.txt"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk>=3.4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "\r\u001b[K     |▎                               | 10kB 14.9MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 1.5MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 2.0MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |████▏                           | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |████▋                           | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 327kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 337kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 368kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 378kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 389kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 399kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 409kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 419kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 430kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 440kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 460kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 471kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 481kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 491kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 501kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 512kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 522kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 532kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 542kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 552kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 563kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 573kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 583kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 593kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 604kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 614kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 624kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 634kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 645kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 655kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 665kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 675kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 686kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 696kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 706kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 716kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 727kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 737kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 747kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 757kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 768kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 778kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 788kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 798kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 808kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 819kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 829kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 839kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 849kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 860kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 870kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 880kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 890kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 901kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 911kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 921kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 931kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 942kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 952kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 962kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 972kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 983kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 993kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.0MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.1MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4MB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 2.7MB/s \n",
            "\u001b[?25hCollecting razdel>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/15/2c/664223a3924aa6e70479f7d37220b3a658765b9cfe760b4af7ffdc50d38f/razdel-0.5.0-py3-none-any.whl\n",
            "Collecting rusenttokenize>=0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/25/4c/a2f00be5def774a3df2e5387145f1cb54e324607ec4a7e23f573645946e7/rusenttokenize-0.0.5-py3-none-any.whl\n",
            "Collecting b-labs-models>=2017.8.22\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/30/36fdfd1ed4f778b52ebe165941c215349caadc410faa163ccacf06e395b4/b_labs_models-2017.8.22-py2.py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 18.7MB/s \n",
            "\u001b[?25hCollecting lxml>=4.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/37/d420b7fdc9a550bd29b8cfeacff3b38502d9600b09d7dfae9a69e623b891/lxml-4.5.2-cp36-cp36m-manylinux1_x86_64.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 30.2MB/s \n",
            "\u001b[?25hCollecting spacy>=2.1.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/b5/c7a92c7ce5d4b353b70b4b5b4385687206c8b230ddfe08746ab0fd310a3a/spacy-2.3.2-cp36-cp36m-manylinux1_x86_64.whl (9.9MB)\n",
            "\u001b[K     |████████████████████████████████| 10.0MB 38.4MB/s \n",
            "\u001b[?25hCollecting pymystem3>=0.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/00/8c/98b43c5822620458704e187a1666616c1e21a846ede8ffda493aabe11207/pymystem3-0.2.0-py3-none-any.whl\n",
            "Collecting rnnmorph>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/b4/c776a30c7ee91715b8c66cc21d87e0ab7952794aa343fefc243cc805f421/rnnmorph-0.4.0.tar.gz (10.5MB)\n",
            "\u001b[K     |████████████████████████████████| 10.5MB 213kB/s \n",
            "\u001b[?25hCollecting click\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/3d/fa76db83bf75c4f8d338c2fd15c8d33fdd7ad23a9b5e57eb6c5de26b430e/click-7.1.2-py2.py3-none-any.whl (82kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 6.3MB/s \n",
            "\u001b[?25hCollecting joblib\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/51/dd/0e015051b4a27ec5a58b02ab774059f3289a94b0906f880a3f9507e74f38/joblib-0.16.0-py3-none-any.whl (300kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 45.1MB/s \n",
            "\u001b[?25hCollecting regex\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/f2/b3af9ce9df4b7e121dfeece41fc95e37b14f0153821f35d08edb0b0813ff/regex-2020.7.14-cp36-cp36m-manylinux2010_x86_64.whl (660kB)\n",
            "\u001b[K     |████████████████████████████████| 665kB 44.5MB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/28/7e/281edb5bc3274dfb894d90f4dbacfceaca381c2435ec6187a2c6f329aed7/tqdm-4.48.2-py2.py3-none-any.whl (68kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 6.1MB/s \n",
            "\u001b[?25hCollecting python-crfsuite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/99/869dde6dbf3e0d07a013c8eebfb0a3d30776334e0097f8432b631a9a3a19/python_crfsuite-0.9.7-cp36-cp36m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 43.7MB/s \n",
            "\u001b[?25hCollecting numpy>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b1/9a/7d474ba0860a41f771c9523d8c4ea56b084840b5ca4092d96bdee8a3b684/numpy-1.19.1-cp36-cp36m-manylinux2010_x86_64.whl (14.5MB)\n",
            "\u001b[K     |████████████████████████████████| 14.5MB 308kB/s \n",
            "\u001b[?25hCollecting thinc==7.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/ae/ef3ae5e93639c0ef8e3eb32e3c18341e511b3c515fcfc603f4b808087651/thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 35.8MB/s \n",
            "\u001b[?25hCollecting catalogue<1.1.0,>=0.0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
            "Collecting wasabi<1.1.0,>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/d1/a23917773a5759b36d1dc8433d15fb40700ca29d5ba924d6350c38a8ef8e/wasabi-0.7.1.tar.gz\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/86/85/40b8f66c2dd8f4fd9f09d59b22720cffecf1331e788b8a0cab5bafb353d1/plac-1.1.3-py2.py3-none-any.whl\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0e/9a/70bd934dd4d25545c9aa6c8cd4edbac2a33ba9c915439a9209b69f0ec0ad/srsly-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (185kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 43.7MB/s \n",
            "\u001b[?25hCollecting setuptools\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/16/e9f5c5b86696da09298ea10c32d68ad8ea21f888e45b11aa9e615adda6c9/setuptools-49.2.1-py3-none-any.whl (789kB)\n",
            "\u001b[K     |████████████████████████████████| 798kB 44.9MB/s \n",
            "\u001b[?25hCollecting requests<3.0.0,>=2.13.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/1e/0c169c6a5381e241ba7404532c16a21d86ab872c9bed8bdcd4c423954103/requests-2.24.0-py2.py3-none-any.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 7.2MB/s \n",
            "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/b5/3e1714ebda8fd7c5859f9b216e381adc0a38b962f071568fd00d67e1b1ca/cymem-2.0.3-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/6b/e07fad36913879757c90ba03d6fb7f406f7279e11dcefc105ee562de63ea/preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119kB)\n",
            "\u001b[K     |████████████████████████████████| 122kB 49.1MB/s \n",
            "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a6/e6/63f160a4fdf0e875d16b28f972083606d8d54f56cd30cb8929f9a1ee700e/murmurhash-1.0.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting blis<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/19/f95c75562d18eb27219df3a3590b911e78d131b68466ad79fdf5847eaac4/blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 42.7MB/s \n",
            "\u001b[?25hCollecting scipy>=0.18.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/a8/f4c66eb529bb252d50e83dbf2909c6502e2f857550f22571ed8556f62d95/scipy-1.5.2-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9MB 1.3MB/s \n",
            "\u001b[?25hCollecting scikit-learn>=0.18.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/a1/273def87037a7fb010512bbc5901c31cfddfca8080bc63b42b26e3cc55b3/scikit_learn-0.23.2-cp36-cp36m-manylinux1_x86_64.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 35.9MB/s \n",
            "\u001b[?25hCollecting tensorflow>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/97/ae/0b08f53498417914f2274cc3b5576d2b83179b0cbb209457d0fde0152174/tensorflow-2.3.0-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 46kB/s \n",
            "\u001b[?25hCollecting keras>=2.0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/44/e1/dc0757b20b56c980b5553c1b5c4c32d378c7055ab7bfa92006801ad359ab/Keras-2.4.3-py2.py3-none-any.whl\n",
            "Collecting pymorphy2>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/33/fff9675c68b5f6c63ec8c6e6ff57827dda28a1fa5b2c2d727dffff92dd47/pymorphy2-0.8-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.1MB/s \n",
            "\u001b[?25hCollecting russian-tagsets==0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/2d/b1/c9377d472a04fb9b84f59365560d68b5d868b589691f32545eb606b3be48/russian-tagsets-0.6.tar.gz\n",
            "Collecting jsonpickle>=0.9.4\n",
            "  Downloading https://files.pythonhosted.org/packages/af/ca/4fee219cc4113a5635e348ad951cf8a2e47fed2e3342312493f5b73d0007/jsonpickle-1.4.1-py2.py3-none-any.whl\n",
            "Collecting importlib-metadata>=0.20; python_version < \"3.8\"\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/58/cdea07eb51fc2b906db0968a94700866fc46249bdc75cac23f9d13168929/importlib_metadata-1.7.0-py2.py3-none-any.whl\n",
            "Collecting idna<3,>=2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a2/38/928ddce2273eaa564f6f50de919327bf3a00f091b5baba8dfa9460f3a8a8/idna-2.10-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 6.7MB/s \n",
            "\u001b[?25hCollecting chardet<4,>=3.0.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 48.1MB/s \n",
            "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/f0/a391d1463ebb1b233795cabfc0ef38d3db4442339de68f847026199e69d7/urllib3-1.25.10-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 43.0MB/s \n",
            "\u001b[?25hCollecting certifi>=2017.4.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl (156kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 39.9MB/s \n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f7/12/ec3f2e203afa394a149911729357aa48affc59c20e2c1c8297a60f33f133/threadpoolctl-2.1.0-py3-none-any.whl\n",
            "Collecting gast==0.3.3\n",
            "  Downloading https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n",
            "Collecting keras-preprocessing<1.2,>=1.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.9MB/s \n",
            "\u001b[?25hCollecting tensorboard<3,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/1b/6a420d7e6ba431cf3d51b2a5bfa06a958c4141e3189385963dc7f6fbffb6/tensorboard-2.3.0-py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 38.0MB/s \n",
            "\u001b[?25hCollecting wheel>=0.26\n",
            "  Downloading https://files.pythonhosted.org/packages/8c/23/848298cccf8e40f5bbb59009b32848a4c38f4e7f3364297ab3c3e2e2cd14/wheel-0.34.2-py2.py3-none-any.whl\n",
            "Collecting six>=1.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n",
            "Collecting termcolor>=1.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
            "Collecting opt-einsum>=2.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.3MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1\n",
            "  Downloading https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n",
            "Collecting grpcio>=1.8.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/42/262913f967217874ae66734b52077833e2153b7b3a55a45bf996c7ee4833/grpcio-1.30.0-cp36-cp36m-manylinux2010_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 41.2MB/s \n",
            "\u001b[?25hCollecting protobuf>=3.9.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/14/dc43f81adc543c435cfeb45dd4ac048a97a1eb621c2ccb68ab3d15118737/protobuf-3.12.4-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 34.0MB/s \n",
            "\u001b[?25hCollecting h5py<2.11.0,>=2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/06/cafdd44889200e5438b897388f3075b52a8ef01f28a17366d91de0fa2d05/h5py-2.10.0-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 41.3MB/s \n",
            "\u001b[?25hCollecting astunparse==1.6.3\n",
            "  Downloading https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n",
            "Collecting google-pasta>=0.1.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl (57kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 7.3MB/s \n",
            "\u001b[?25hCollecting absl-py>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 24.3MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator<2.4.0,>=2.3.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/ed/5853ec0ae380cba4588eab1524e18ece1583b65f7ae0e97321f5ff9dfd60/tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 43.7MB/s \n",
            "\u001b[?25hCollecting pyyaml\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 46.1MB/s \n",
            "\u001b[?25hCollecting docopt>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
            "Collecting dawg-python>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Collecting pymorphy2-dicts<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/51/2465fd4f72328ab50877b54777764d928da8cb15b74e2680fc1bd8cb3173/pymorphy2_dicts-2.4.393442.3710985-py2.py3-none-any.whl (7.1MB)\n",
            "\u001b[K     |████████████████████████████████| 7.1MB 36.0MB/s \n",
            "\u001b[?25hCollecting zipp>=0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/b2/34/bfcb43cc0ba81f527bc4f40ef41ba2ff4080e047acb0586b56b3d017ace4/zipp-3.1.0-py3-none-any.whl\n",
            "Collecting markdown>=2.6.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/63/eaec2bd025ab48c754b55e8819af0f6a69e2b1e187611dd40cbbe101ee7f/Markdown-3.2.2-py3-none-any.whl (88kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.7MB/s \n",
            "\u001b[?25hCollecting google-auth-oauthlib<0.5,>=0.4.1\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/b8/88def36e74bee9fce511c9519571f4e485e890093ab7442284f4ffaef60b/google_auth_oauthlib-0.4.1-py2.py3-none-any.whl\n",
            "Collecting tensorboard-plugin-wit>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b6/85/5c5ac0a8c5efdfab916e9c6bc18963f6a6996a8a1e19ec4ad8c9ac9c623c/tensorboard_plugin_wit-1.7.0-py3-none-any.whl (779kB)\n",
            "\u001b[K     |████████████████████████████████| 788kB 43.3MB/s \n",
            "\u001b[?25hCollecting werkzeug>=0.11.15\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
            "\u001b[K     |████████████████████████████████| 307kB 50.1MB/s \n",
            "\u001b[?25hCollecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cb/02/00e06ffa98fd0f11f36f808511012fa1fce41e4f79fa35dc7c515364ed01/google_auth-1.20.0-py2.py3-none-any.whl (91kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 10.1MB/s \n",
            "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/12/b92740d845ab62ea4edf04d2f4164d82532b5a0b03836d4d4e71c6f3d379/requests_oauthlib-1.3.0-py2.py3-none-any.whl\n",
            "Collecting cachetools<5.0,>=2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/5c/f3aa86b6d5482f3051b433c7616668a9b96fbe49a622210e2c9781938a5c/cachetools-4.1.1-py3-none-any.whl\n",
            "Collecting rsa<5,>=3.1.4; python_version >= \"3.5\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1c/df/c3587a667d6b308fadc90b99e8bc8774788d033efcc70f4ecaae7fad144b/rsa-4.6-py3-none-any.whl (47kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 5.6MB/s \n",
            "\u001b[?25hCollecting pyasn1-modules>=0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/de/214830a981892a3e286c3794f41ae67a4495df1108c3da8a9f62159b9a9d/pyasn1_modules-0.2.8-py2.py3-none-any.whl (155kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 50.0MB/s \n",
            "\u001b[?25hCollecting oauthlib>=3.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/05/57/ce2e7a8fa7c0afb54a0581b14a65b56e62b5759dbc98e80627142b8a3704/oauthlib-3.1.0-py2.py3-none-any.whl (147kB)\n",
            "\u001b[K     |████████████████████████████████| 153kB 42.7MB/s \n",
            "\u001b[?25hCollecting pyasn1>=0.1.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 8.6MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: nltk, rnnmorph, wasabi, russian-tagsets, termcolor, wrapt, absl-py, pyyaml, docopt\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434676 sha256=c129a5684cbb51713bf80c271c13b5db1b620a1a9e418ddc849279b8cc04a841\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "  Building wheel for rnnmorph (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rnnmorph: filename=rnnmorph-0.4.0-cp36-none-any.whl size=10521037 sha256=c31162780821f945807e7eb16a4e779fe244eb37c331b54b2c58d66ba681c999\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/74/5d/3c6c523a759b67e6a81677e2aad003321536587d1575a4face\n",
            "  Building wheel for wasabi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wasabi: filename=wasabi-0.7.1-cp36-none-any.whl size=20836 sha256=210637b3edf2bd7248484fed31257bcf5b108e776fed40a2b01dd0b3aff5038c\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/44/92/957b5b17132d849926c598305bdc72dae678ecb79bc4ae6ba0\n",
            "  Building wheel for russian-tagsets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for russian-tagsets: filename=russian_tagsets-0.6-cp36-none-any.whl size=24636 sha256=0532217714ba2e96efdf64b889e9990d72d2a21d3eda5ad0d59004f6fd4f36e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/e8/9d/dd/4679aca4031fdb0d3ad65e165ba5343e61441ed7ad587a08e6\n",
            "  Building wheel for termcolor (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for termcolor: filename=termcolor-1.1.0-cp36-none-any.whl size=4832 sha256=dbc13fa145fbd5e7cefb86edad93bb706e8654f49f4e7398030883a4c73b5c83\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/06/54/bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp36-cp36m-linux_x86_64.whl size=67502 sha256=7afbc8e6277b9597fda7cdd62e386f8b89cef4c264a4c82735edc8f94cc0f0f9\n",
            "  Stored in directory: /root/.cache/pip/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n",
            "  Building wheel for absl-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for absl-py: filename=absl_py-0.9.0-cp36-none-any.whl size=121931 sha256=21760782d7a63e40fe334901fe7c3b5283251836fc40fa0aa390aef197f349a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
            "  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=f3addbde11c8b251e68913ab4d07637e7ca37b5a4471c159b6eac3b7efd2756f\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13704 sha256=b6cc7dae613d1933b6ed234380c70ccea3e1f7a1d27fa1c3f2a5d9055604ac31\n",
            "  Stored in directory: /root/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
            "Successfully built nltk rnnmorph wasabi russian-tagsets termcolor wrapt absl-py pyyaml docopt\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement numpy<1.19.0,>=1.16.0, but you'll have numpy 1.19.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement scipy==1.4.1, but you'll have scipy 1.5.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kaggle 1.5.6 has requirement urllib3<1.25,>=1.21.1, but you'll have urllib3 1.25.10 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.17.2, but you'll have google-auth 1.20.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement requests~=2.23.0, but you'll have requests 2.24.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: click, joblib, regex, tqdm, nltk, razdel, rusenttokenize, python-crfsuite, b-labs-models, lxml, numpy, zipp, importlib-metadata, catalogue, cymem, murmurhash, preshed, srsly, wasabi, plac, blis, thinc, setuptools, idna, chardet, urllib3, certifi, requests, spacy, pymystem3, scipy, threadpoolctl, scikit-learn, gast, six, keras-preprocessing, markdown, wheel, protobuf, absl-py, oauthlib, requests-oauthlib, cachetools, pyasn1, rsa, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard-plugin-wit, werkzeug, grpcio, tensorboard, termcolor, opt-einsum, wrapt, h5py, astunparse, google-pasta, tensorflow-estimator, tensorflow, pyyaml, keras, docopt, dawg-python, pymorphy2-dicts, pymorphy2, russian-tagsets, jsonpickle, rnnmorph\n",
            "\u001b[33m  WARNING: The script tqdm is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script nltk is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script razdel-ctl is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts f2py, f2py3 and f2py3.6 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts easy_install and easy_install-3.6 are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script chardetect is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script markdown_py is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script wheel is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts pyrsa-decrypt, pyrsa-encrypt, pyrsa-keygen, pyrsa-priv2pub, pyrsa-sign and pyrsa-verify are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script google-oauthlib-tool is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The script tensorboard is installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "\u001b[33m  WARNING: The scripts estimator_ckpt_converter, saved_model_cli, tensorboard, tf_upgrade_v2, tflite_convert, toco and toco_from_protos are installed in '/root/.local/bin' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
            "Successfully installed absl-py-0.9.0 astunparse-1.6.3 b-labs-models-2017.8.22 blis-0.4.1 cachetools-4.1.1 catalogue-1.0.0 certifi-2020.6.20 chardet-3.0.4 click-7.1.2 cymem-2.0.3 dawg-python-0.7.2 docopt-0.6.2 gast-0.3.3 google-auth-1.20.0 google-auth-oauthlib-0.4.1 google-pasta-0.2.0 grpcio-1.30.0 h5py-2.10.0 idna-2.10 importlib-metadata-1.7.0 joblib-0.16.0 jsonpickle-1.4.1 keras-2.4.3 keras-preprocessing-1.1.2 lxml-4.5.2 markdown-3.2.2 murmurhash-1.0.2 nltk-3.5 numpy-1.19.1 oauthlib-3.1.0 opt-einsum-3.3.0 plac-1.1.3 preshed-3.0.2 protobuf-3.12.4 pyasn1-0.4.8 pyasn1-modules-0.2.8 pymorphy2-0.8 pymorphy2-dicts-2.4.393442.3710985 pymystem3-0.2.0 python-crfsuite-0.9.7 pyyaml-5.3.1 razdel-0.5.0 regex-2020.7.14 requests-2.24.0 requests-oauthlib-1.3.0 rnnmorph-0.4.0 rsa-4.6 rusenttokenize-0.0.5 russian-tagsets-0.6 scikit-learn-0.23.2 scipy-1.5.2 setuptools-49.2.1 six-1.15.0 spacy-2.3.2 srsly-1.0.2 tensorboard-2.3.0 tensorboard-plugin-wit-1.7.0 tensorflow-2.3.0 tensorflow-estimator-2.3.0 termcolor-1.1.0 thinc-7.4.1 threadpoolctl-2.1.0 tqdm-4.48.2 urllib3-1.25.10 wasabi-0.7.1 werkzeug-1.0.1 wheel-0.34.2 wrapt-1.12.1 zipp-3.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cachetools",
                  "certifi",
                  "chardet",
                  "google",
                  "grpc",
                  "idna",
                  "numpy",
                  "pkg_resources",
                  "pyasn1",
                  "pyasn1_modules",
                  "requests",
                  "rsa",
                  "six",
                  "tqdm",
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FJ2-w6keDYv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "6f2423d4-4195-427f-f496-9a276a19226f"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Dqa3TH-eDYx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 632
        },
        "outputId": "1921d224-66bb-428f-b58c-99535bd0f618"
      },
      "source": [
        "import sys\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting en_core_web_sm==2.3.1\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0MB)\n",
            "\u001b[K     |████████████████████████████████| 12.1MB 787kB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /root/.local/lib/python3.6/site-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.24.0)\n",
            "Requirement already satisfied: thinc==7.4.1 in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.48.2)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.19.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.2.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /root/.local/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /root/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /root/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /root/.local/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.25.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /root/.local/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /root/.local/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.1.0)\n",
            "Building wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-cp36-none-any.whl size=12047111 sha256=6f03765ca34835d2b20f5e0dbaea7534b3e74686d9639ea5a99c9c96e4362f49\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-_ihhsno_/wheels/2b/3f/41/f0b92863355c3ba34bb32b37d8a0c662959da0058202094f46\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Found existing installation: en-core-web-sm 2.2.5\n",
            "    Uninstalling en-core-web-sm-2.2.5:\n",
            "      Successfully uninstalled en-core-web-sm-2.2.5\n",
            "Successfully installed en-core-web-sm-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A7fL9saeDYz",
        "colab_type": "text"
      },
      "source": [
        "### Дополнительные данные\n",
        "\n",
        "Opencorpora: 31 Мб по сети, 530 Мб в распакованном виде"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BnW_XqBeDY0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "a15996d1-8163-4638-ce46-193353f631fc"
      },
      "source": [
        "!wget http://opencorpora.org/files/export/annot/annot.opcorpora.xml.bz2"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-05 04:17:45--  http://opencorpora.org/files/export/annot/annot.opcorpora.xml.bz2\n",
            "Resolving opencorpora.org (opencorpora.org)... 148.251.2.141\n",
            "Connecting to opencorpora.org (opencorpora.org)|148.251.2.141|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 32766652 (31M) [application/x-bzip2]\n",
            "Saving to: ‘annot.opcorpora.xml.bz2’\n",
            "\n",
            "annot.opcorpora.xml 100%[===================>]  31.25M  12.2MB/s    in 2.6s    \n",
            "\n",
            "2020-08-05 04:17:48 (12.2 MB/s) - ‘annot.opcorpora.xml.bz2’ saved [32766652/32766652]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wX8NtUveDY2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!bzip2 -d annot.opcorpora.xml.bz2"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiKkGUnbeDY4",
        "colab_type": "text"
      },
      "source": [
        "### Тестовые примеры"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdqD-e7beDY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "example1 = \"this's a sent tokenize test. this is sent two. is this sent three? sent 4 is cool! Now it’s your turn.\"\n",
        "example2 = \"\"\"\n",
        "    An ambitious campus expansion plan was proposed by Fr. Vernon F. Gallagher in 1952.\n",
        "    Assumption Hall, the first student dormitory, was opened in 1954,\n",
        "    and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.\n",
        "    It was during the tenure of F. Henry J. McAnulty that Fr. Gallagher's ambitious plans were put to action.\n",
        "\"\"\"\n",
        "example3 = \"\"\"\n",
        "    А что насчёт русского языка? Хорошо ли сегментируются имена?\n",
        "    Ай да А.С. Пушкин! Ай да сукин сын!\n",
        "    «Как же так?! Захар...» — воскликнут Пронин.\n",
        "    - \"Так в чем же дело?\" - \"Не ра-ду-ют\".\n",
        "    И т. д. и т. п. В общем, вся газета.\n",
        "    Православие... более всего подходит на роль такой идеи...\n",
        "    Нефть за $27/барр. не снится.\n",
        "\"\"\"\n",
        "example4 = \"\"\"\n",
        "    Кружка-термос на 0.5л (50/64 см³, 516;...) стоит $3.88\n",
        "\"\"\"\n",
        "example5 = \"\"\"\n",
        "    Good muffins cost $3.88 in New York.  Please buy me two of them. Thanks.\n",
        "\"\"\""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cqtzsw4eDY7",
        "colab_type": "text"
      },
      "source": [
        "# Сегментация предложений\n",
        "Первая задача - разбиение текста на предложения"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGebcOPseDY8",
        "colab_type": "text"
      },
      "source": [
        "### Экперименты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiEkV_MdeDY9",
        "colab_type": "text"
      },
      "source": [
        "##### NLTK - Natural Language Toolkit\n",
        "Популярная платформа для анализа текстов. Особенно хорошо работает для английского. В основном не содержит ничего из машинного обучения, только старые добрые правила."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42DWKe5FeDY9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2291c613-6530-44fa-e6ba-d0f024fe1ae0"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sent_tokenize(example1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"this's a sent tokenize test.\",\n",
              " 'this is sent two.',\n",
              " 'is this sent three?',\n",
              " 'sent 4 is cool!',\n",
              " 'Now it’s your turn.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWpr39pPeDZA",
        "colab_type": "text"
      },
      "source": [
        "А вот тут что-то пошло не так"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8bXtc-6eDZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "d34cfe35-9c8b-4731-8ce5-0d1ad2d3ff9a"
      },
      "source": [
        "sent_tokenize(example2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n    An ambitious campus expansion plan was proposed by Fr.',\n",
              " 'Vernon F. Gallagher in 1952.',\n",
              " 'Assumption Hall, the first student dormitory, was opened in 1954,\\n    and Rockwell Hall was dedicated in November 1958, housing the schools of business and law.',\n",
              " 'It was during the tenure of F. Henry J. McAnulty that Fr.',\n",
              " \"Gallagher's ambitious plans were put to action.\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dpmBe70eDZE",
        "colab_type": "text"
      },
      "source": [
        "А что насчёт русского языка?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUNCCwnzeDZF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "95f698bb-9685-499f-dd7d-edc20c53baff"
      },
      "source": [
        "sent_tokenize(example3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n    А что насчёт русского языка?',\n",
              " 'Хорошо ли сегментируются имена?',\n",
              " 'Ай да А.С.',\n",
              " 'Пушкин!',\n",
              " 'Ай да сукин сын!',\n",
              " '«Как же так?!',\n",
              " 'Захар...» — воскликнут Пронин.',\n",
              " '- \"Так в чем же дело?\"',\n",
              " '- \"Не ра-ду-ют\".',\n",
              " 'И т. д. и т. п. В общем, вся газета.',\n",
              " 'Православие... более всего подходит на роль такой идеи...\\n    Нефть за $27/барр.',\n",
              " 'не снится.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rWZ0bF4eDZH",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/Mottl/ru_punkt\n",
        "\n",
        "Data for sentence tokenization was taken from 3 sources:\n",
        "\n",
        "  * Articles from Russian Wikipedia (about 1 million sentences)\n",
        "  * Common Russian abbreviations from Russian orthographic dictionary, edited by V. V. Lopatin;\n",
        "  * Generated names initials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nwr9BaZ3eDZI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "eba3f882-6502-45f0-9ae7-aec03601e513"
      },
      "source": [
        "sent_tokenize(example3, language=\"russian\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n    А что насчёт русского языка?',\n",
              " 'Хорошо ли сегментируются имена?',\n",
              " 'Ай да А.С. Пушкин!',\n",
              " 'Ай да сукин сын!',\n",
              " '«Как же так?!',\n",
              " 'Захар...» — воскликнут Пронин.',\n",
              " '- \"Так в чем же дело?\"',\n",
              " '- \"Не ра-ду-ют\".',\n",
              " 'И т. д. и т. п. В общем, вся газета.',\n",
              " 'Православие... более всего подходит на роль такой идеи...\\n    Нефть за $27/барр.',\n",
              " 'не снится.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erWAIVigeDZL",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/natasha/razdel\n",
        "\n",
        "razdel старается разбивать текст на предложения и токены так, как это сделано в 4 датасетах: SynTagRus, OpenCorpora, ГИКРЯ и РНК из репозитория morphoRuEval-2017.\n",
        "\n",
        "В основном это новостные тексты и литература. Правила razdel заточены под них.\n",
        "\n",
        "На текстах другой тематики (социальные сети, научные статьи) библиотека может работать хуже."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1xW9spMIaHE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "663480d7-fee4-4bca-d709-ffd693c44d7c"
      },
      "source": [
        "!pip install razdel"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: razdel in /root/.local/lib/python3.6/site-packages (0.5.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q6pOD9BJ4I6Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, '/root/.local/lib/python3.6/site-packages')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iY1qpHh6eDZL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "d1dfdcf8-02a8-47ec-8452-91f62fb3d3e6"
      },
      "source": [
        "from razdel import sentenize\n",
        "list(sentenize(example3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Substring(5, 33, 'А что насчёт русского языка?'),\n",
              " Substring(34, 65, 'Хорошо ли сегментируются имена?'),\n",
              " Substring(70, 88, 'Ай да А.С. Пушкин!'),\n",
              " Substring(89, 105, 'Ай да сукин сын!'),\n",
              " Substring(110, 123, '«Как же так?!'),\n",
              " Substring(124, 154, 'Захар...» — воскликнут Пронин.'),\n",
              " Substring(159, 181, '- \"Так в чем же дело?\"'),\n",
              " Substring(182, 198, '- \"Не ра-ду-ют\".'),\n",
              " Substring(203, 218, 'И т. д. и т. п.'),\n",
              " Substring(219, 239, 'В общем, вся газета.'),\n",
              " Substring(244,\n",
              "           301,\n",
              "           'Православие... более всего подходит на роль такой идеи...'),\n",
              " Substring(306, 335, 'Нефть за $27/барр. не снится.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6DDGryLeDZP",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/deepmipt/ru_sentence_tokenizer\n",
        "    \n",
        "A simple and fast rule-based sentence segmentation. Tested on OpenCorpora and SynTagRus datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6iYyZ1TeDZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "6d9cc8f8-a4a6-464b-e2e9-89ff4b5b120c"
      },
      "source": [
        "from rusenttokenize import ru_sent_tokenize\n",
        "ru_sent_tokenize(example3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Something went wrong while tokenizing\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['А что насчёт русского языка?',\n",
              " 'Хорошо ли сегментируются имена?',\n",
              " 'Ай да А.С. Пушкин!',\n",
              " 'Ай да сукин сын!',\n",
              " '«Как же так?!',\n",
              " 'Захар...» — воскликнут Пронин.',\n",
              " '- \"Так в чем же дело?\"',\n",
              " '- \"Не ра-ду-ют\".',\n",
              " 'И т. д. и т. п.',\n",
              " 'В общем, вся газета.',\n",
              " 'Православие... более всего подходит на роль такой идеи...',\n",
              " 'Нефть за $27/барр. не снится.',\n",
              " '']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwNAtibneDZS",
        "colab_type": "text"
      },
      "source": [
        "### Бенчмарки\n",
        "Много вариантов... Нужно измерять"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwae8cmleDZT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b8b97f2b-b8d3-4094-d3ae-a541b4110f90"
      },
      "source": [
        "# WARNING: RAM bound task, XML parsing is expensive\n",
        "# Similar to https://github.com/deepmipt/ru_sentence_tokenizer/blob/master/metrics/calculate.ipynb\n",
        "import re\n",
        "from lxml import etree\n",
        "\n",
        "# \\W -> Any non-word character\n",
        "RE_ENDS_WITH_PUNCT = re.compile(r\".*\\W$\")\n",
        "\n",
        "OPENCORPORA_FILE = \"annot.opcorpora.xml\"\n",
        "sentences = list(etree.parse(OPENCORPORA_FILE).xpath('//source/text()'))\n",
        "singles = []\n",
        "compounds = []\n",
        "s2 = sentences.pop().strip()\n",
        "singles.append(s2)\n",
        "while sentences:\n",
        "    s1 = sentences.pop().strip()\n",
        "    singles.append(s1)\n",
        "    if RE_ENDS_WITH_PUNCT.match(s1) and not s1.endswith(':') and not s2.startswith('—'):\n",
        "        compounds.append((s1, s2))\n",
        "    s2 = s1\n",
        "        \n",
        "print(f'Read {len(singles)} sentences from {OPENCORPORA_FILE}')\n",
        "        \n",
        "del sentences"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 110302 sentences from annot.opcorpora.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2havDX5seDZV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_sent_tokenizer(tokenizer, singles, compounds):\n",
        "    correct_count_in_singles = 0\n",
        "    for sentence in singles:\n",
        "        correct_count_in_singles += len(tokenizer(sentence)) == 1\n",
        "\n",
        "    correct_count_in_compounds = 0\n",
        "    for s1, s2 in compounds:\n",
        "        correct_count_in_compounds += tokenizer(s1 + ' ' + s2) == [s1, s2]\n",
        "\n",
        "    return (correct_count_in_singles / len(singles), correct_count_in_compounds / len(compounds))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvWnP6bEeDZY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d8398acd-e1a1-498f-aa95-22ba8cc65c99"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(sent_tokenize, singles, compounds)\n",
        "print(f'sent_tokenizer scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 12.7 s, sys: 3.92 ms, total: 12.7 s\n",
            "Wall time: 12.7 s\n",
            "sent_tokenizer scores: 94.27%, 86.04%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9f9m7nnneDZc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f8bd3908-6870-4eb3-c1ef-90073f13d64e"
      },
      "source": [
        "russian_sent_tokenize = lambda s : sent_tokenize(s, language=\"russian\")\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(russian_sent_tokenize, singles, compounds)\n",
        "print(f'russian sent_tokenizer scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 12.7 s, sys: 0 ns, total: 12.7 s\n",
            "Wall time: 12.7 s\n",
            "russian sent_tokenizer scores: 96.73%, 88.81%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MClqfLRveDZh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d52a0ec1-dd2c-459c-85e7-9647086cd2dc"
      },
      "source": [
        "from razdel import sentenize\n",
        "razdel_sent_tokenize = lambda text : [s.text for s in sentenize(text)]\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(razdel_sent_tokenize, singles, compounds)\n",
        "print(f'razdel scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 6.3 s, sys: 0 ns, total: 6.3 s\n",
            "Wall time: 6.3 s\n",
            "razdel scores: 99.07%, 95.39%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IuAOcawmeDZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a93a35aa-ebf1-4d97-cb26-8f86b56e9184"
      },
      "source": [
        "from rusenttokenize import ru_sent_tokenize\n",
        "deepmipt_sent_tokenize = ru_sent_tokenize\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(deepmipt_sent_tokenize, singles, compounds)\n",
        "print(f'deepmipt scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 7.88 s, sys: 0 ns, total: 7.88 s\n",
            "Wall time: 7.89 s\n",
            "deepmipt scores: 98.73%, 93.42%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0tQJPC51eDZs",
        "colab_type": "text"
      },
      "source": [
        "Аналогичные бенчмарки:\n",
        "- https://github.com/natasha/razdel/blob/master/eval.ipynb\n",
        "- https://github.com/deepmipt/ru_sentence_tokenizer/blob/master/metrics/calculate.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zmDYwWleDZt",
        "colab_type": "text"
      },
      "source": [
        "### Задание 1: \"Кирпич\"\n",
        "Скачайте предложенный текст. Найдите первое предложение, которое отличается в разбиениях, порождённых rusenttokenize и razdel. Верните номер этого предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKi00K-seDZu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "ba455b07-7181-40ad-85f9-1f41bb2af0bc"
      },
      "source": [
        "!wget https://www.dropbox.com/s/q5wo34gfbepc7am/htbg.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-08-05 11:27:17--  https://www.dropbox.com/s/q5wo34gfbepc7am/htbg.txt\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.82.1, 2620:100:6032:1::a27d:5201\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.82.1|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/q5wo34gfbepc7am/htbg.txt [following]\n",
            "--2020-08-05 11:27:17--  https://www.dropbox.com/s/raw/q5wo34gfbepc7am/htbg.txt\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc8adf41c2fd846fcb40df196d44.dl.dropboxusercontent.com/cd/0/inline/A87SUAenionD6zKmbxRQxWelw_8QIzxGq3NmMYTSDqT7leqx6Cp86ufpju0wPSD-4nS-pgGp3m-yOyOFWS7CqYhkEE48FK2-SmIBBOU_clSSiw/file# [following]\n",
            "--2020-08-05 11:27:17--  https://uc8adf41c2fd846fcb40df196d44.dl.dropboxusercontent.com/cd/0/inline/A87SUAenionD6zKmbxRQxWelw_8QIzxGq3NmMYTSDqT7leqx6Cp86ufpju0wPSD-4nS-pgGp3m-yOyOFWS7CqYhkEE48FK2-SmIBBOU_clSSiw/file\n",
            "Resolving uc8adf41c2fd846fcb40df196d44.dl.dropboxusercontent.com (uc8adf41c2fd846fcb40df196d44.dl.dropboxusercontent.com)... 162.125.82.15, 2620:100:6032:15::a27d:520f\n",
            "Connecting to uc8adf41c2fd846fcb40df196d44.dl.dropboxusercontent.com (uc8adf41c2fd846fcb40df196d44.dl.dropboxusercontent.com)|162.125.82.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 592726 (579K) [text/plain]\n",
            "Saving to: ‘htbg.txt’\n",
            "\n",
            "\rhtbg.txt              0%[                    ]       0  --.-KB/s               \rhtbg.txt            100%[===================>] 578.83K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2020-08-05 11:27:18 (8.13 MB/s) - ‘htbg.txt’ saved [592726/592726]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLzXHecqeDZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from razdel import sentenize\n",
        "from rusenttokenize import ru_sent_tokenize\n",
        "\n",
        "with open(\"htbg.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2v0gej649pV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "740984a6-f718-43ba-9f3f-f716bb7c9501"
      },
      "source": [
        "for num, (i, j) in enumerate(zip(ru_sent_tokenize(text), sentenize(text))):\n",
        "  _,_, s = j\n",
        "  if s!=i:\n",
        "    print('Num: {}'.format(num))\n",
        "    print('Razdel: {}'.format(s))\n",
        "    print('Russent: {}'.format(i))\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Something went wrong while tokenizing\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Num: 329\n",
            "Razdel: – Мудры были предки,– задумчиво сказал Пашка.– Этак едешь-едешь километров двести, вдруг – хлоп! – «кирпич».\n",
            "Russent: – Мудры были предки,– задумчиво сказал Пашка.– Этак едешь-едешь километров двести, вдруг – хлоп!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQq4iPN8eDZ2",
        "colab_type": "text"
      },
      "source": [
        "### Задание 2: Lazy baseline\n",
        "Напишите свой sent_tokenize, который будет делить предложения только по точкам, восклицательным и вопросительным знакам. Измерьте для него время работы и метрики на opencorpora."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "47iRVTmqeDZ3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3d18e8fd-a7c8-494c-9bea-44d9f472f37f"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "import re\n",
        "def my_sent_tokenize(text):\n",
        "    return re.split('(?<=\\.) |(?<=\\!) |(?<=\\?) ', text)\n",
        "\n",
        "assert my_sent_tokenize(example1) == sent_tokenize(example1)\n",
        "%time singles_score, compounds_score = check_sent_tokenizer(sent_tokenize, singles, compounds)\n",
        "assert singles_score >= 0.85\n",
        "print(f'your scores: {singles_score*100:.2f}%, {compounds_score*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 12.5 s, sys: 12.6 ms, total: 12.5 s\n",
            "Wall time: 12.6 s\n",
            "your scores: 94.27%, 86.04%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvte2OBUeDZ5",
        "colab_type": "text"
      },
      "source": [
        "# Токенизация\n",
        "\n",
        "Самый наивный способ токенизировать текст -- разделить с помощью split. Но split упускает очень много всего, например, банально не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем. Поэтому лучше использовать готовые токенизаторы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0BjSKCneDZ5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "40522886-6f3e-4d6f-8d5b-592e20f243f5"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(example5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30jl5PpZeDZ8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "f670e512-f9f4-4512-8d8a-67bd4bdea9ca"
      },
      "source": [
        "from nltk import tokenize\n",
        "dir(tokenize)[:16]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer',\n",
              " 'TreebankWordTokenizer',\n",
              " 'TweetTokenizer',\n",
              " 'WhitespaceTokenizer',\n",
              " 'WordPunctTokenizer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD3EUO5WeDZ_",
        "colab_type": "text"
      },
      "source": [
        "Они умеют выдавать индексы начала и конца каждого токена:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2z_ZybxeDaA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30edb959-0c14-4220-9826-0703d461343f"
      },
      "source": [
        "from nltk import tokenize\n",
        "wh_tok = tokenize.WhitespaceTokenizer()\n",
        "print(list(wh_tok.span_tokenize(example5)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(5, 9), (10, 17), (18, 22), (23, 28), (29, 31), (32, 35), (36, 41), (43, 49), (50, 53), (54, 56), (57, 60), (61, 63), (64, 69), (70, 77)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4HjdWCceDaE",
        "colab_type": "text"
      },
      "source": [
        "Некторые токенизаторы ведут себя специфично:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyzF35cOeDaG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aed8ce14-4f2b-4ef2-ac02-7a7a702390d9"
      },
      "source": [
        "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do', \"n't\", 'stop', 'me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQCmuQpGeDaK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3c7d438f-ba56-4d00-b40a-b13bf5f90304"
      },
      "source": [
        "import spacy\n",
        "spacy_nlp = spacy.load('en_core_web_sm')\n",
        "doc = spacy_nlp(example5, disable=[\"parser\"])\n",
        "print([token.text for token in doc])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\n    ', 'Good', 'muffins', 'cost', '$', '3.88', 'in', 'New', 'York', '.', ' ', 'Please', 'buy', 'me', 'two', 'of', 'them', '.', 'Thanks', '.', '\\n']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yg9w4wNSeDaN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4b5e6ffe-7773-4732-97bb-0293cddffb3a"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "print(word_tokenize(example4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Кружка-термос', 'на', '0.5л', '(', '50/64', 'см³', ',', '516', ';', '...', ')', 'стоит', '$', '3.88']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVvJnHfKeDaP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "891528e0-3e9b-49fb-d14f-bc0c80ea37c0"
      },
      "source": [
        "from razdel import tokenize\n",
        "list(tokenize(example4))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Substring(5, 18, 'Кружка-термос'),\n",
              " Substring(19, 21, 'на'),\n",
              " Substring(22, 25, '0.5'),\n",
              " Substring(25, 26, 'л'),\n",
              " Substring(27, 28, '('),\n",
              " Substring(28, 33, '50/64'),\n",
              " Substring(34, 37, 'см³'),\n",
              " Substring(37, 38, ','),\n",
              " Substring(39, 42, '516'),\n",
              " Substring(42, 43, ';'),\n",
              " Substring(43, 46, '...'),\n",
              " Substring(46, 47, ')'),\n",
              " Substring(48, 53, 'стоит'),\n",
              " Substring(54, 55, '$'),\n",
              " Substring(55, 59, '3.88')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVCUdgpOeDaR",
        "colab_type": "text"
      },
      "source": [
        "### Задание 3: Diff\n",
        "Напишите функцию, которая будет выводить разницу между токенизацией razdel'а и nltk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gRApSXtKeDaS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from razdel import tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "with open(\"htbg.txt\", \"r\") as f:\n",
        "    text = f.read()\n",
        "    \n",
        "def get_tokenization_differences(text: str) -> int:\n",
        "    text_1 = tokenize(text)\n",
        "    text_10 = []\n",
        "    for i in text_1:\n",
        "      _,_,t = i\n",
        "      text_10.append(t)\n",
        "    text_2 = word_tokenize(text)\n",
        "    differences = []\n",
        "    s = SequenceMatcher(None, text_10, text_2)\n",
        "    for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
        "      if tag in ['insert', 'delete', 'replace']:\n",
        "        differences.append(text_2[j1:j2])\n",
        "    return differences\n",
        "    \n",
        "assert len(get_tokenization_differences(text)) == 613"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IPBbpoVeDaU",
        "colab_type": "text"
      },
      "source": [
        "# Стоп-слова и пунктуация\n",
        "\n",
        "Стоп-слова - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конкретном документе, то есть играют роль шума. Поэтому их принято убирать. По той же причине убирают и пунктуацию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CwruKoseDaU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8a51f5a4-5de1-40ae-88e8-da51c51f965c"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "print(stopwords.words('russian'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PVbyxz-eDaW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ddddfaa-454e-46af-8d7d-5f2eacd4cf1d"
      },
      "source": [
        "from string import punctuation\n",
        "punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rksBLbveDaZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "noise = stopwords.words('russian') + list(punctuation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAbQc-VzeDab",
        "colab_type": "text"
      },
      "source": [
        "### Задание 4: Стоп-слова from scratch\n",
        "Постройте свой список стоп-слов на основе Opencorpora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yGRerPDseDac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e5771c1-6dbf-4a8b-fb7b-a4c424a36f29"
      },
      "source": [
        "import re\n",
        "from lxml import etree\n",
        "\n",
        "OPENCORPORA_FILE = \"annot.opcorpora.xml\"\n",
        "sentences = list(etree.parse(OPENCORPORA_FILE).xpath('//source/text()'))\n",
        "print(f'Read {len(sentences)} sentences from {OPENCORPORA_FILE}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read 110302 sentences from annot.opcorpora.xml\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOuKlXTjkhRi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "vocab = Counter(xs for s in sentences for xs in word_tokenize(s))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWKR8EH2liLh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_vords = vocab.most_common()[:200]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkGQgO59KZ9G",
        "colab_type": "text"
      },
      "source": [
        "# Стемминг"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZsLS4NQKW77",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "31c39e47-e41b-4c9a-f075-ef8bae456750"
      },
      "source": [
        "from nltk.stem.snowball import SnowballStemmer \n",
        "from razdel import tokenize\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\") \n",
        "print([stemmer.stem(token.text) for token in tokenize(example3)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['а', 'что', 'насчет', 'русск', 'язык', '?', 'хорош', 'ли', 'сегментир', 'им', '?', 'а', 'да', 'а', '.', 'с', '.', 'пушкин', '!', 'а', 'да', 'сукин', 'сын', '!', '«', 'как', 'же', 'так', '?!', 'захар', '...', '»', '—', 'воскликнут', 'пронин', '.', '-', '\"', 'так', 'в', 'чем', 'же', 'дел', '?', '\"', '-', '\"', 'не', 'ра-ду-ют', '\"', '.', 'и', 'т', '.', 'д', '.', 'и', 'т', '.', 'п', '.', 'в', 'общ', ',', 'вся', 'газет', '.', 'православ', '...', 'бол', 'всег', 'подход', 'на', 'рол', 'так', 'ид', '...', 'нефт', 'за', '$', '27', '/', 'барр', '.', 'не', 'снит', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FM6ufubEeDae",
        "colab_type": "text"
      },
      "source": [
        "# Лемматизация и морфологический анализ\n",
        "\n",
        "Лемматизация – это сведение разных форм одного слова к начальной форме – лемме. Почему это хорошо?\n",
        "* Мы хотим рассматривать как отдельную фичу каждое слово, а не каждую его отдельную форму.\n",
        "* Некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n",
        "\n",
        "Для русского есть два хороших лемматизатора: mystem и pymorphy. С pymorphy всё сразу понятно.\n",
        "\n",
        "Но как работать с Mystem:\n",
        "* Можно скачать mystem и запускать из терминала с разными параметрами\n",
        "* pymystem3 - обертка для питона, работает медленнее, но это удобно"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mfork2jueDaf",
        "colab_type": "text"
      },
      "source": [
        "## Mystem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1ZoKAS5eDag",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pymystem3 import Mystem\n",
        "mystem_analyzer = Mystem()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7McqUjduMYxR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod +x /root/.local/bin/mystem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsSzbzVWeDai",
        "colab_type": "text"
      },
      "source": [
        "Мы инициализировали Mystem c дефолтными параметрами. А вообще параметры есть такие:\n",
        "\n",
        "    mystem_bin - путь к mystem, если их несколько\n",
        "    grammar_info - нужна ли грамматическая информация или только леммы (по дефолту нужна)\n",
        "    disambiguation - нужно ли снятие омонимии - дизамбигуация (по дефолту нужна)\n",
        "    entire_input - нужно ли сохранять в выводе все (пробелы всякие, например), или можно выкинуть (по дефолту оставляется все)\n",
        "\n",
        "Методы Mystem принимают строку, токенизатор вшит внутри. Можно, конечно, и пословно анализировать, но тогда он не сможет учитывать контекст.\n",
        "\n",
        "Можно просто лемматизировать текст:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK6iAkmWeDai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(mystem_analyzer.lemmatize(example3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZg3McyqeDal",
        "colab_type": "text"
      },
      "source": [
        "А можно получить грамматическую информацию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-F_PNsqneDal",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mystem_analyzer.analyze(example3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJH0dy1YeDan",
        "colab_type": "text"
      },
      "source": [
        "## Pymorphy\n",
        "\n",
        "Это модуль на питоне, довольно быстрый и с кучей функций."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5tsjLpjeDao",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOudNTMJeDap",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "268e6d5b-f7d7-44ad-b261-712a8cec5d05"
      },
      "source": [
        "pymorphy2_analyzer.parse(\"мою\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parse(word='мою', tag=OpencorporaTag('ADJF,Apro femn,sing,accs'), normal_form='мой', score=0.666666, methods_stack=((<DictionaryAnalyzer>, 'мою', 1967, 10),)),\n",
              " Parse(word='мою', tag=OpencorporaTag('VERB,impf,tran sing,1per,pres,indc'), normal_form='мыть', score=0.333333, methods_stack=((<DictionaryAnalyzer>, 'мою', 1813, 1),))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8qJvkxveDas",
        "colab_type": "text"
      },
      "source": [
        "## mystem vs. pymorphy\n",
        "\n",
        "1) Mystem работает невероятно медленно под windows на больших текстах.\n",
        "\n",
        "2) Снятие омонимии. Mystem умеет снимать омонимию по контексту (хотя не всегда преуспевает), pymorphy2 берет на вход одно слово и соответственно вообще не умеет дизамбигуировать по контексту:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GO55wLFkeDas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "homonym1 = 'За время обучения я прослушал больше сорока курсов.'\n",
        "homonym2 = 'Сорока своровала блестящее украшение со стола.'\n",
        "mystem_analyzer = Mystem() # инициализирую объект с дефолтными параметрами\n",
        "\n",
        "print(mystem_analyzer.analyze(homonym1)[-5])\n",
        "print(mystem_analyzer.analyze(homonym2)[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GApdUILveDax",
        "colab_type": "text"
      },
      "source": [
        "## Rnnmorph\n",
        "Обёртка над pymorphy с разрешением омонимии\n",
        "\n",
        "https://github.com/IlyaGusev/rnnmorph\n",
        "\n",
        "https://habr.com/ru/post/339954/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nUqNO1leDa8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4cbf09c5-d164-4b40-9a7d-3a9375b7f895"
      },
      "source": [
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "from razdel import tokenize\n",
        "\n",
        "predictor = RNNMorphPredictor(language=\"ru\")\n",
        "homonym = \"Косил косой косой косой\"\n",
        "print(predictor.predict([t.text for t in tokenize(homonym)])[1])\n",
        "print(predictor.predict([t.text for t in tokenize(homonym)])[-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<normal_form=косой; word=косой; pos=ADJ; tag=Case=Nom|Degree=Pos|Gender=Masc|Number=Sing; score=0.4093>\n",
            "<normal_form=коса; word=косой; pos=NOUN; tag=Case=Ins|Gender=Fem|Number=Sing; score=0.7904>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvFlNWZ1eDbA",
        "colab_type": "text"
      },
      "source": [
        "### Задание 5: Формат\n",
        "\n",
        "Используя стандартные инструменты переведите корпус htbg.txt в формат CoNLL-U.\n",
        "Используйте следующие колонки: \n",
        "    1. Номер предложения в тексте\n",
        "    2. Токен в том виде, в котором он встретился в тексте\n",
        "    3. Лемма токена\n",
        "    4. POS-таг токена\n",
        "    5. Вектор грамматических значений токена\n",
        "    6. Целевая метка (сделайте метку везде OUT)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kt3EriRa6uxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"htbg.txt\", \"r\") as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qeax1MWpYh7l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/root/.local/lib/python3.6/site-packages')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBoUCkT66u7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from rnnmorph.predictor import RNNMorphPredictor\n",
        "from razdel import sentenize, tokenize\n",
        "\n",
        "predictor = RNNMorphPredictor(language=\"ru\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28Ya0ygR7X8q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = [i.text for i in sentenize(text)]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gm4EsKR886P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "60065c08-7dab-4b7a-8439-c68a0612ed79"
      },
      "source": [
        "conll = []\n",
        "for num, i in enumerate(texts):\n",
        "  words = []\n",
        "  for t in tokenize(i):\n",
        "    words.append((num, t.text))\n",
        "  conll.append(predictor.predict(words))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/.local/lib/python3.6/site-packages/rnnmorph/batch_generator.py:175: RuntimeWarning: invalid value encountered in true_divide\n",
            "  gram_value_indices[index:index + len(values)] = mask / s\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qfew_CvgeDbB",
        "colab_type": "text"
      },
      "source": [
        "# Regex 101"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDYSLv67eDbB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPMiIDMEeDbE",
        "colab_type": "text"
      },
      "source": [
        "#### match\n",
        "ищет по заданному шаблону в начале строки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJ4RY9dveDbF",
        "colab_type": "code",
        "colab": {},
        "outputId": "3edae44a-c6f7-4d87-911a-8ed821d9446e"
      },
      "source": [
        "result = re.match('ab+c.', 'abcdefghijkabcabc') # ищем по шаблону 'ab+c.' \n",
        "print (result) # совпадение найдено:"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_sre.SRE_Match object; span=(0, 4), match='abcd'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S2qN9Q1keDbH",
        "colab_type": "code",
        "colab": {},
        "outputId": "274ef809-dad2-4d06-ad24-27058f104a41"
      },
      "source": [
        "print(result.group(0)) # выводим найденное совпадение"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abcd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tqg4wISLeDbK",
        "colab_type": "code",
        "colab": {},
        "outputId": "30749fcd-3adb-47d9-b00a-e06ed3561cae"
      },
      "source": [
        "result = re.match('abc.', 'abdefghijkabcabc')\n",
        "print(result) # совпадение не найдено"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dop3xhkteDbN",
        "colab_type": "text"
      },
      "source": [
        "#### search\n",
        "ищет по всей строке, возвращает только первое найденное совпадение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsP6-GN-eDbO",
        "colab_type": "code",
        "colab": {},
        "outputId": "01479a68-4ebb-44d2-dc32-7cd00a40089f"
      },
      "source": [
        "result = re.search('ab+c.', 'aefgabchijkabcabc') \n",
        "print(result) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<_sre.SRE_Match object; span=(4, 8), match='abch'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqd_IVNNeDbS",
        "colab_type": "text"
      },
      "source": [
        "#### findall\n",
        "возвращает список всех найденных совпадений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox0yWO57eDbT",
        "colab_type": "code",
        "colab": {},
        "outputId": "c76ae674-c236-4c13-ec15-ae356af9a222"
      },
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abcd', 'abca']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4ncd6YPeDbV",
        "colab_type": "text"
      },
      "source": [
        "Вопросы: \n",
        "1) почему нет последнего abc?\n",
        "2) почему нет abcx?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BN71xIw6eDbY",
        "colab_type": "text"
      },
      "source": [
        "#### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsWWmhUKeDbZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGR8y8uCeDbc",
        "colab_type": "text"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alEOle-XeDbd",
        "colab_type": "code",
        "colab": {},
        "outputId": "b9a8a422-b7f0-42ae-885b-c8223c5f6e7a"
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit = 2) \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['itsy', ' bitsy', ' teenie, weenie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDIt0zSKeDbg",
        "colab_type": "text"
      },
      "source": [
        "#### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d79CKEy5eDbk",
        "colab_type": "code",
        "colab": {},
        "outputId": "e80bb469-a416-4a26-f57d-f387482e8af3"
      },
      "source": [
        "result = re.sub('a', 'b', 'abcabc')\n",
        "print (result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bbcbbc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhBHsBTyeDbo",
        "colab_type": "text"
      },
      "source": [
        "#### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6r3PuZ3feDbp",
        "colab_type": "code",
        "colab": {},
        "outputId": "2eda1655-fa30-4956-ef45-2f4d3412f0cd"
      },
      "source": [
        "prog = re.compile('[А-Яа-яё\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    }
  ]
}